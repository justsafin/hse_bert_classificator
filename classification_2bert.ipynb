{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, BertModel, BertTokenizer, BertForSequenceClassification\n",
    "import transformers\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_dataset(df=None, path_to_csv=''):\n",
    "    def get_one_level_labels(field):\n",
    "        new_field_title = field + \"_proc\"\n",
    "        df[new_field_title] = df[field].apply(literal_eval)\n",
    "        mlb = MultiLabelBinarizer()\n",
    "        sparce_labels = mlb.fit_transform(df[new_field_title])\n",
    "        level_classes = mlb.classes_\n",
    "        print(f\"Unique {field} classes: {len(level_classes)}\")\n",
    "\n",
    "        return level_classes, sparce_labels\n",
    "\n",
    "    if df is None:\n",
    "        df = pd.read_csv(path_to_csv, sep=\"\\t\")\n",
    "\n",
    "    l1_classes, l1_labels = get_one_level_labels(\"RGNTI_L1\")\n",
    "    l2_classes, l2_labels = get_one_level_labels(\"RGNTI_L2\")\n",
    "\n",
    "    return df[\"data\"].to_numpy(), l1_classes, l1_labels, l2_classes, l2_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextsDataset(Dataset):\n",
    "    def __init__(self, tokenizer, texts, labels_l1, labels_l2):\n",
    "        self.texts = texts\n",
    "        self.labels_l1 = labels_l1\n",
    "        self.labels_l2 = labels_l2\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels_l1.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            self.texts[idx],\n",
    "            None,\n",
    "            pad_to_max_length=True,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=256,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        label_l1 = torch.from_numpy(self.labels_l1[idx]).to(dtype=torch.float32)\n",
    "        label_l2 = torch.from_numpy(self.labels_l2[idx]).to(dtype=torch.float32)\n",
    "        return {\n",
    "            'ids': inputs[\"input_ids\"].squeeze(),\n",
    "            'mask': inputs[\"attention_mask\"].squeeze(),\n",
    "            'token_type_ids': inputs[\"token_type_ids\"].squeeze(),\n",
    "        }, label_l1, label_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoBertClassifier(nn.Module):\n",
    "    def __init__(self, embedding_model_path, l1_classes: int, l2_classes: int, freeze_bert=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedder_l1 = BertModel.from_pretrained(embedding_model_path)\n",
    "        self.embedder_l2 = BertModel.from_pretrained(embedding_model_path)\n",
    "\n",
    "        self.freeze_bert = freeze_bert\n",
    "        if self.freeze_bert:\n",
    "            for param in self.embedder_l1.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.embedder_l2.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.dropout_l1 = nn.Dropout(0.2)\n",
    "        self.classifier_l1 = nn.Linear(768, l1_classes)\n",
    "\n",
    "        self.dropout_l2 = nn.Dropout(0.2)\n",
    "        self.classifier_l2 = nn.Linear(768 + l1_classes, l2_classes)\n",
    "\n",
    "    def set_eval(self):\n",
    "        self.eval()\n",
    "        for param in self.embedder_l1.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        for param in self.embedder_l2.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def set_train(self):\n",
    "        self.train()\n",
    "        if self.freeze_bert:\n",
    "            for param in self.embedder_l1.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in self.embedder_l2.parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            for param in self.embedder_l1.parameters():\n",
    "                param.requires_grad = True\n",
    "            for param in self.embedder_l2.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        x = self.embedder_l1(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
    "        x = self.dropout_l1(x.pooler_output)\n",
    "        l1_logits = self.classifier_l1(x)\n",
    "\n",
    "        x = self.embedder_l2(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
    "        x = self.dropout_l2(x.pooler_output)\n",
    "        x = torch.cat((x, l1_logits), dim=1)\n",
    "        l2_logits = self.classifier_l2(x)\n",
    "\n",
    "        return l1_logits, l2_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=-1, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        p = torch.sigmoid(inputs)\n",
    "        bce_loss = torch.nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = p * targets + (1 - p) * (1 - targets)\n",
    "        loss = (1 - pt) ** self.gamma * bce_loss\n",
    "        if self.alpha >= 0:\n",
    "            alpha_t = self.alpha * targets + (1 - self.alpha) * (1 - targets)\n",
    "            loss = alpha_t * loss\n",
    "        loss = loss.mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_num = 2\n",
    "device = \"cuda\"\n",
    "batch_size = 36\n",
    "initial_lr = 1e-5\n",
    "# resume_path = \"exps/experiment_1727880964/checkpoints/model_0003.tar\"\n",
    "resume_path = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folders\n",
    "experiment_path = Path(f\"exps/experiment_{int(time.time())}\")\n",
    "experiment_path.mkdir(parents=True, exist_ok=True)\n",
    "checkpoints_path = experiment_path.joinpath(\"checkpoints\")\n",
    "checkpoints_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "log_path = experiment_path.joinpath(\"logs\")\n",
    "log_path.mkdir(parents=True, exist_ok=True)\n",
    "writer = SummaryWriter(log_path.absolute().as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"C:\\\\PowerfullProject\\\\teach_slice_80_l2_drop_wasted_v2.csv\", sep=\"\\t\")\n",
    "df_test = pd.read_csv(\"C:\\\\PowerfullProject\\\\test_slice_20_l2_drop_wasted_v2.csv\", sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([df_train, df_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique RGNTI_L1 classes: 46\n",
      "Unique RGNTI_L2 classes: 463\n"
     ]
    }
   ],
   "source": [
    "all_data, l1_classes, l1_labels, l2_classes, l2_labels = get_data_from_dataset(df=result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(599248,)\n",
      "(46,)\n",
      "(599248, 46)\n",
      "(463,)\n",
      "(599248, 463)\n"
     ]
    }
   ],
   "source": [
    "print(all_data.shape)\n",
    "print(l1_classes.shape)\n",
    "print(l1_labels.shape)\n",
    "print(l2_classes.shape)\n",
    "print(l2_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(514795, 463)\n",
      "(84453, 463)\n"
     ]
    }
   ],
   "source": [
    "train_data = all_data[:514795]\n",
    "l1_train_classes = l1_classes\n",
    "l2_train_classes = l2_classes\n",
    "l1_train_labels = l1_labels[:514795]\n",
    "l2_train_labels = l2_labels[:514795]\n",
    "\n",
    "test_data = all_data[514795:]\n",
    "l1_test_classes = l1_classes\n",
    "l2_test_classes = l2_classes\n",
    "l1_test_labels = l1_labels[514795:]\n",
    "l2_test_labels = l2_labels[514795:]\n",
    "\n",
    "\n",
    "print(l2_train_labels.shape)\n",
    "print(l2_test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate classes\n",
    "print(\"Read labels\")\n",
    "\n",
    "train_data, l1_train_classes, l1_train_labels, l2_train_classes, l2_train_labels = get_data_from_dataset(\"Dataset/slices_80x20_drop_wasted/teach_slice_80_l2_drop_wasted_v2.csv\")\n",
    "test_data, l1_test_classes, l1_test_labels, l2_test_classes, l2_test_labels = get_data_from_dataset(\"Dataset/slices_80x20_drop_wasted/test_slice_20_l2_drop_wasted_v2.csv\")\n",
    "\n",
    "assert (l1_train_classes == l1_test_classes).all(), \"Train and test classes on L1 do not match\"\n",
    "assert (l2_train_classes == l2_test_classes).all(), \"Train and test classes on L2 do not match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at miemBertProject/miem-scibert-linguistic and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at miemBertProject/miem-scibert-linguistic and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TwoBertClassifier(\n",
      "  (embedder_l1): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (embedder_l2): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout_l1): Dropout(p=0.2, inplace=False)\n",
      "  (classifier_l1): Linear(in_features=768, out_features=46, bias=True)\n",
      "  (dropout_l2): Dropout(p=0.2, inplace=False)\n",
      "  (classifier_l2): Linear(in_features=814, out_features=463, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instant model, optimizer, etc.\n",
    "print(\"Creating model\")\n",
    "# embedding_model_name = \"ai-forever/ruBert-large\"\n",
    "embedding_model_name = \"miemBertProject/miem-scibert-linguistic\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)\n",
    "model = TwoBertClassifier(embedding_model_name, l1_classes=len(l1_train_classes), l2_classes=len(l2_train_classes), freeze_bert=False)\n",
    "if resume_path:\n",
    "    print(f\"Loading model from {resume_path}\")\n",
    "    checkpoint = torch.load(resume_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "print(model)\n",
    "with open(experiment_path.joinpath(\"model_structure.txt\"), \"w\") as f:\n",
    "    f.writelines(str(model))\n",
    "model.to(device)\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "# loss_fn = FocalLoss()\n",
    "loss_fn.to(device)\n",
    "optimizer = transformers.AdamW(model.parameters(), lr=initial_lr)\n",
    "\n",
    "# Create Datasets and Dataloaders\n",
    "training_data = TextsDataset(tokenizer, train_data, l1_train_labels, l2_train_labels)\n",
    "test_data = TextsDataset(tokenizer, train_data, l1_test_labels, l2_test_labels)\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "total_steps = len(train_dataloader)*epoch_num\n",
    "warm_up_end = 5000\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=initial_lr, total_steps=total_steps, pct_start=warm_up_end/total_steps, div_factor=1e2)\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "#     optimizer, [1, 2, 3, 6, 8], gamma=0.5, verbose=True)\n",
    "\n",
    "min_val_loss = np.inf\n",
    "max_val_acc = 0\n",
    "max_val_f1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   train[1/2][2024-10-06-11:42]:   0%|                                                            | 1/14300 [02:12<524:26:17, 132.04s/it, train_loss=1.4142, train_l1_loss=0.7223, train_l2_loss=0.6920]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m train_bar\u001b[38;5;241m.\u001b[39mpostfix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mstep\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, train_l1_loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_l1_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mstep\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, train_l2_loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_l2_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mstep\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     36\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 37\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad_norm_value)\u001b[39;00m\n\u001b[0;32m     39\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\PowerfullProject\\.venv\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\PowerfullProject\\.venv\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Start training\")\n",
    "for epoch in range(epoch_num):\n",
    "    model.set_train()\n",
    "    total_loss = 0\n",
    "    total_l1_loss = 0\n",
    "    total_l2_loss = 0\n",
    "    train_bar = tqdm(train_dataloader, ncols=200)\n",
    "    \n",
    "    # Train epoch\n",
    "    for step, (input, true_l1_labels, true_l2_labels) in enumerate(train_bar, 1):\n",
    "        true_l1_labels = true_l1_labels.to(device)\n",
    "        true_l2_labels = true_l2_labels.to(device)\n",
    "    \n",
    "        ids = input[\"ids\"].to(device)\n",
    "        mask = input[\"mask\"].to(device)\n",
    "        token_type_ids = input[\"token_type_ids\"].to(device)\n",
    "    \n",
    "        pred_l1_labels, pred_l2_labels = model(\n",
    "            ids=ids,\n",
    "            mask=mask,\n",
    "            token_type_ids=token_type_ids)\n",
    "    \n",
    "        loss_l1 = loss_fn(pred_l1_labels, true_l1_labels)\n",
    "        loss_l2 = loss_fn(pred_l2_labels, true_l2_labels)\n",
    "        loss = loss_l1 + loss_l2\n",
    "    \n",
    "        total_loss += loss.item()\n",
    "        total_l1_loss += loss_l1.item()\n",
    "        total_l2_loss += loss_l2.item()\n",
    "    \n",
    "        train_bar.desc = '   train[{}/{}][{}]'.format(\n",
    "            epoch+1, epoch_num, datetime.now().strftime(\"%Y-%m-%d-%H:%M\"))\n",
    "    \n",
    "        train_bar.postfix = f'train_loss={total_loss / step:.4f}, train_l1_loss={total_l1_loss / step:.4f}, train_l2_loss={total_l2_loss / step:.4f}'\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad_norm_value)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        if step % 1500 == 0:\n",
    "            writer.add_scalars('train_loss', {'train_loss': total_loss / step}, step+epoch*len(train_dataloader))\n",
    "            writer.add_scalars('train_loss', {'train_l1_loss': total_l1_loss / step}, step+epoch*len(train_dataloader))\n",
    "            writer.add_scalars('train_loss', {'train_l2_loss': total_l2_loss / step}, step+epoch*len(train_dataloader))\n",
    "            writer.add_scalars('lr', {'lr': optimizer.param_groups[0]['lr']}, step+epoch*len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "validate[1/2][2024-10-06-00:53]: 100%|████████████████| 14300/14300 [2:20:14<00:00,  1.70it/s, valid_loss=1.3944, l1_acc=0.000, l2_acc=0.000, l1_f1_weighted=14.08, l2_f1_weighted=5.81, l2_f1_micro=0.55, l2_f1_macro=0.42]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best vall loss = 1.3944133786888389\n",
      "New best vall f1 = 1.3944133786888389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "validate[2/2][2024-10-06-03:12]: 100%|████████████████| 14300/14300 [2:19:37<00:00,  1.71it/s, valid_loss=1.3944, l1_acc=0.000, l2_acc=0.000, l1_f1_weighted=14.11, l2_f1_weighted=5.83, l2_f1_micro=0.55, l2_f1_macro=0.43]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best vall loss = 1.39441336889367\n",
      "New best vall f1 = 1.39441336889367\n",
      "Best acc: 0, best f1: 0.0054809096975621556, best val loss: 1.39441336889367\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch_num):\n",
    "    # Validation\n",
    "    model.set_eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    l1_total_loss = 0\n",
    "    l1_total_acc = 0\n",
    "    l1_total_f1 = 0\n",
    "    l1_total_f1_macro = 0\n",
    "    l1_total_f1_weighted = 0\n",
    "\n",
    "    l2_total_loss = 0\n",
    "    l2_total_acc = 0\n",
    "    l2_total_f1 = 0\n",
    "    l2_total_f1_macro = 0\n",
    "    l2_total_f1_weighted = 0\n",
    "\n",
    "    validation_bar = tqdm(train_dataloader, ncols=220)\n",
    "    with torch.no_grad():\n",
    "        for step, (input, true_l1_labels, true_l2_labels) in enumerate(validation_bar, 1):\n",
    "            true_l1_labels = true_l1_labels.to(device)\n",
    "            true_l2_labels = true_l2_labels.to(device)\n",
    "\n",
    "            ids = input[\"ids\"].to(device)\n",
    "            mask = input[\"mask\"].to(device)\n",
    "            token_type_ids = input[\"token_type_ids\"].to(device)\n",
    "\n",
    "            pred_l1_labels, pred_l2_labels = model(\n",
    "                ids=ids,\n",
    "                mask=mask,\n",
    "                token_type_ids=token_type_ids)\n",
    "\n",
    "            loss_l1 = loss_fn(pred_l1_labels, true_l1_labels)\n",
    "            loss_l2 = loss_fn(pred_l2_labels, true_l2_labels)\n",
    "            loss = loss_l1 + loss_l2\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            l1_total_loss += loss_l1.item()\n",
    "            l2_total_loss += loss_l2.item()\n",
    "\n",
    "            validation_bar.desc = 'validate[{}/{}][{}]'.format(\n",
    "                epoch+1, epoch_num, datetime.now().strftime(\"%Y-%m-%d-%H:%M\"))\n",
    "\n",
    "            def calculate_level_metrics(pred_labels, true_labels):\n",
    "                pred_labels_cpu = torch.sigmoid(pred_labels).detach().cpu().numpy()\n",
    "                pred_labels_cpu = np.where(pred_labels_cpu<0.5, 0, 1)\n",
    "\n",
    "                true_labels_cpu = true_labels.detach().cpu().numpy()\n",
    "                accuracy = accuracy_score(y_true=true_labels_cpu, y_pred=pred_labels_cpu)\n",
    "                f1 = f1_score(y_true=true_labels_cpu, y_pred=pred_labels_cpu, average=\"micro\")\n",
    "                f1_macro = f1_score(y_true=true_labels_cpu, y_pred=pred_labels_cpu, average=\"macro\")\n",
    "                f1_weighted = f1_score(y_true=true_labels_cpu, y_pred=pred_labels_cpu, average=\"weighted\")\n",
    "                return accuracy, f1, f1_macro, f1_weighted\n",
    "\n",
    "\n",
    "            accuracy, f1, f1_macro, f1_weighted = calculate_level_metrics(pred_l1_labels, true_l1_labels)\n",
    "            l1_total_acc += accuracy\n",
    "            l1_total_f1 += f1\n",
    "            l1_total_f1_macro += f1_macro\n",
    "            l1_total_f1_weighted += f1_weighted\n",
    "\n",
    "            accuracy, f1, f1_macro, f1_weighted = calculate_level_metrics(pred_l2_labels, true_l2_labels)\n",
    "            l2_total_acc += accuracy\n",
    "            l2_total_f1 += f1\n",
    "            l2_total_f1_macro += f1_macro\n",
    "            l2_total_f1_weighted += f1_weighted\n",
    "\n",
    "            validation_bar.postfix = (f'valid_loss={total_loss / step:.4f}, '\n",
    "                                        f'l1_acc={(l1_total_acc / step) * 100:.3f}, '\n",
    "                                        f'l2_acc={(l2_total_acc / step) * 100:.3f}, '\n",
    "                                        f'l1_f1_weighted={(l1_total_f1_weighted / step) * 100:.2f}, '\n",
    "                                        f'l2_f1_weighted={(l2_total_f1_weighted / step) * 100:.2f}, '\n",
    "                                        f'l2_f1_micro={(l2_total_f1 / step) * 100:.2f}, '\n",
    "                                        f'l2_f1_macro={(l2_total_f1_macro / step) * 100:.2f}')\n",
    "\n",
    "    epoch_val_loss = total_loss / step\n",
    "    epoch_val_acc = l2_total_acc / step\n",
    "    epoch_val_f1 = l2_total_f1 / step\n",
    "    epoch_val_f1_macro = l2_total_f1_macro / step\n",
    "    epoch_val_f1_weighted = l2_total_f1_weighted / step\n",
    "    writer.add_scalars(\n",
    "        'val_loss', {'val_loss': epoch_val_loss,\n",
    "                        'acc': epoch_val_acc,\n",
    "                        'f1': epoch_val_f1,\n",
    "                        'f1_macro': epoch_val_f1_macro,\n",
    "                        'f1_weighted': epoch_val_f1_weighted}, epoch)\n",
    "\n",
    "\n",
    "    # Save checkpoint\n",
    "    model_dict = model.state_dict()\n",
    "    state_dict = {'epoch': epoch,\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'scheduler': scheduler.state_dict(),\n",
    "                    'model': model_dict}\n",
    "\n",
    "    torch.save(state_dict, checkpoints_path.joinpath(f'model_{str(epoch).zfill(4)}.tar').as_posix())\n",
    "\n",
    "    if epoch_val_loss < min_val_loss:\n",
    "        min_val_loss = epoch_val_loss\n",
    "        # torch.save(state_dict, checkpoints_path.joinpath('best_val_loss_model.tar').as_posix())\n",
    "        print(f'New best vall loss = {min_val_loss}')\n",
    "\n",
    "    if epoch_val_acc > max_val_acc:\n",
    "        max_val_acc = epoch_val_acc\n",
    "        torch.save(state_dict, checkpoints_path.joinpath('best_val_acc_model.tar').as_posix())\n",
    "        print(f'New best vall acc = {max_val_acc} achived on {epoch} epoch. Model saved.')\n",
    "\n",
    "    if epoch_val_f1 > max_val_f1:\n",
    "        max_val_f1 = epoch_val_f1\n",
    "        # torch.save(state_dict, checkpoints_path.joinpath('best_val_f1_model.tar').as_posix())\n",
    "        print(f'New best vall f1 = {min_val_loss}')\n",
    "\n",
    "    # scheduler.step()\n",
    "\n",
    "print(f\"Best acc: {max_val_acc}, best f1: {max_val_f1}, best val loss: {min_val_loss}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
